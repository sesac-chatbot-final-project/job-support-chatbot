import os
import re
import time
import pymysql
import requests
from dotenv import load_dotenv
import pandas as pd
import chromedriver_autoinstaller
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime
import pytz

# í•œêµ­ ì‹œê°„ ì„¤ì •
KST = pytz.timezone("Asia/Seoul")

load_dotenv()

# MariaDB ì—°ê²° ì„¤ì •
db = pymysql.connect(
    host=os.getenv('DB_HOST'),
    port=int(os.getenv('DB_PORT')),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    database=os.getenv('DB_NAME'),
    charset="utf8mb4"
)

def create_saved_jobs_table():
    """ì €ì¥ëœ ê³µê³  í…Œì´ë¸” ìƒì„± ë° ì´ˆê¸°í™”"""
    cursor = db.cursor()

    cursor.execute("DROP TABLE IF EXISTS job_posting_new")
    
    create_table_query = """
    CREATE TABLE job_posting_new (
        id INT AUTO_INCREMENT PRIMARY KEY,
        ì œëª© VARCHAR(255),
        íšŒì‚¬ëª… VARCHAR(255),
        ì‚¬ìš©ê¸°ìˆ  TEXT,
        ê·¼ë¬´ì§€ì—­ VARCHAR(255),
        ê·¼ë¡œì¡°ê±´ VARCHAR(255),
        ëª¨ì§‘ê¸°ê°„ VARCHAR(255),
        ë§í¬ TEXT,
        ì €ì¥ì¼ì‹œ DATETIME DEFAULT CONVERT_TZ(NOW(), 'UTC', 'Asia/Seoul'),
        ì£¼ìš”ì—…ë¬´ TEXT,
        ìê²©ìš”ê±´ TEXT,
        ìš°ëŒ€ì‚¬í•­ TEXT,
        ë³µì§€_ë°_í˜œíƒ TEXT,
        ì±„ìš©ì ˆì°¨ TEXT,
        í•™ë ¥ VARCHAR(255),
        ê·¼ë¬´ì§€ì—­_ìƒì„¸ VARCHAR(255),
        ë§ˆê°ì¼ì VARCHAR(255)
    )
    """
    cursor.execute(create_table_query)
    db.commit()
    cursor.close()

# ğŸ”¹ ì˜ì–´ â†’ í•œê¸€ ë³€í™˜ ë§¤í•‘
eng_to_kor = {
    "Backend": "ë°±ì—”ë“œ",
    "Frontend": "í”„ë¡ íŠ¸ì—”ë“œ",
    "Fullstack": "í’€ìŠ¤íƒ",
    "Engineer": "ì—”ì§€ë‹ˆì–´",
    "Developer": "ê°œë°œì",
    "AI": "ì¸ê³µì§€ëŠ¥",
    "ML": "ë¨¸ì‹ ëŸ¬ë‹",
    "Data": "ë°ì´í„°",
    "Scientist": "ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸",
    "Analyst": "ë¶„ì„ê°€",
    "Cloud": "í´ë¼ìš°ë“œ",
    "DevOps": "ë°ë¸Œì˜µìŠ¤",
    "Security": "ë³´ì•ˆ",
    "Manager": "ë§¤ë‹ˆì €",
    "Lead": "ë¦¬ë“œ",
    "Architect": "ì•„í‚¤í…íŠ¸",
    "Software": "ì†Œí”„íŠ¸ì›¨ì–´",
    "Android": "ì•ˆë“œë¡œì´ë“œ",
    "Python": "íŒŒì´ì¬",
}

def translate_eng_to_kor_with_original(text):
    """ì˜ì–´ë¥¼ í•œê¸€ë¡œ ë³€í™˜í•˜ë©´ì„œ ì›ë˜ ì˜ì–´ë„ í•¨ê»˜ ì €ì¥"""
    pattern = re.compile(r"\b(" + "|".join(eng_to_kor.keys()) + r")\b")
    return pattern.sub(lambda x: f"{eng_to_kor[x.group()]} ({x.group()})", text)

def preprocess_job_details(details):
    """íŠ¹ì • ì»¬ëŸ¼ ì „ì²˜ë¦¬: ë‹¨ì–´ ì‚­ì œ ë° ì±„ìš©ì ˆì°¨ ê°’ ì²˜ë¦¬"""
    remove_words_map = {
        "ê·¼ë¬´ì§€ì—­_ìƒì„¸": ["ì§€ë„ë³´ê¸°Â·ì£¼ì†Œë³µì‚¬"],
    }
    
    for key, words in remove_words_map.items():
        if key in details and details[key] != "ì •ë³´ ì—†ìŒ":
            for word in words:
                details[key] = details[key].replace(word, "").strip()

    # ëª¨ë“  ì»¬ëŸ¼ì—ì„œ ë¹ˆ ê°’, ê³µë°± ë¬¸ìì—´, ê¸¸ì´ê°€ 1 ì´í•˜ë©´ "ì •ë³´ ì—†ìŒ" ì²˜ë¦¬
    for key in details:
        if not details[key].strip() or len(details[key]) <= 1:
            details[key] = "ì •ë³´ ì—†ìŒ"

    return details

# skill ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€
def preprocess_skill(skill_text):
    """skill ë°ì´í„°ë¥¼ Â· ê¸°í˜¸ëŠ” ','ë¡œ ë³€ê²½í•˜ê³ , ì—”í„°ëŠ” ê³µë°±ìœ¼ë¡œ ë³€í™˜"""
    return skill_text.replace("Â·", ",").replace("\n", "").strip()

def scrape_job_details(job_link):
    """ê³µê³  ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
    try:
        response = requests.get(job_link)
        soup = BeautifulSoup(response.text, 'html.parser')

        details = {}
        detail_selectors = {
            'ì£¼ìš”ì—…ë¬´': 'body > main > div > div > section > div.position_info > dl:nth-child(3) > dd > pre',
            'ìê²©ìš”ê±´': 'body > main > div > div > section > div.position_info > dl:nth-child(4) > dd > pre',
            'ìš°ëŒ€ì‚¬í•­': 'body > main > div > div > section > div.position_info > dl:nth-child(5) > dd > pre',
            'ë³µì§€_ë°_í˜œíƒ': 'body > main > div > div > section > div.position_info > dl:nth-child(6) > dd > pre',
            'ì±„ìš©ì ˆì°¨': 'body > main > div > div > section > div.position_info > dl:nth-child(7) > dd > pre',
            'í•™ë ¥': 'body > main > div > div.sc-10492dab-4.kvnCkd > section > div.sc-b12ae455-0.ehVsnD > dl:nth-child(3) > dd',
            'ê·¼ë¬´ì§€ì—­_ìƒì„¸': 'body > main > div > div > section > div.sc-b12ae455-0.ehVsnD > dl:nth-child(5) > dd > ul > li',
            'ë§ˆê°ì¼ì': 'body > main > div > div > section > div.sc-b12ae455-0.ehVsnD > dl:nth-child(4) > dd'
        }

        for key, selector in detail_selectors.items():
            try:
                element = soup.select_one(selector)
                text = element.get_text(strip=True) if element else "ì •ë³´ ì—†ìŒ"
                details[key] = text
            except:
                details[key] = "ì •ë³´ ì—†ìŒ"

        # ì „ì²˜ë¦¬ ì ìš©
        return preprocess_job_details(details)
    
    except Exception as e:
        print(f"ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}


def scrape_jobs():
    """ì±„ìš© ê³µê³  í¬ë¡¤ë§"""
    try:
        chromedriver_autoinstaller.install()

        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        service = Service()
        driver = webdriver.Chrome(service=service, options=chrome_options)

        url = "https://jumpit.saramin.co.kr/positions?career=0&sort=rsp_rate"
        driver.get(url)
        time.sleep(5)

        last_height = driver.execute_script("return document.body.scrollHeight")  
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")  
            time.sleep(2)  
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:  
                break
            last_height = new_height

        job_data = []
        job_elements = driver.find_elements(By.XPATH, "//section/div")
        print(f"ì´ {len(job_elements)}ê°œì˜ ê³µê³ ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        

        for job_element in job_elements:
            try:
                title = WebDriverWait(job_element, 10).until(
                    EC.presence_of_element_located((By.XPATH, ".//a/div[3]/h2"))
                ).text.strip()
                company_name = job_element.find_element(By.XPATH, ".//a/div[3]/div/span").text.strip()
                skill = job_element.find_element(By.XPATH, ".//a/div[3]/ul[1]").text.strip()
                loc = job_element.find_element(By.XPATH, ".//a/div[3]/ul[2]/li[1]").text.strip()
                condition = job_element.find_element(By.XPATH, ".//a/div[3]/ul[2]/li[2]").text.strip()
                date = job_element.find_element(By.XPATH, ".//a/div[2]/div[1]/span").text.strip()
                job_url = job_element.find_element(By.TAG_NAME, "a").get_attribute("href")

                # ì œëª©ì— í•œê¸€ ë³€í™˜ + ì›ë˜ ì˜ì–´ ì¶”ê°€
                title = translate_eng_to_kor_with_original(title)
                skill = preprocess_skill(skill)
                job_data.append((title, company_name, skill, loc, condition, date, job_url))

            except Exception as e:
                print(f"Error: {e}")

        driver.quit()
        return job_data
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def save_to_db(job_data):
    """DBì— í¬ë¡¤ë§í•œ ë°ì´í„° ì €ì¥"""
    cursor = db.cursor()
    saved_job_count = 0
    
    for job in job_data:
        title, company_name, skill, loc, condition, date, job_url = job
        
        # 'D-day'ì¸ ë°ì´í„° ì œì™¸
        if date.strip().lower() == 'd-day':
            print(f"'{title}' ê³µê³ ëŠ” 'D-day'ì´ë¯€ë¡œ ì œì™¸ë¨.")
            continue

        # ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        details = scrape_job_details(job_url)

        insert_query = """
        INSERT INTO job_posting_new (ì œëª©, íšŒì‚¬ëª…, ì‚¬ìš©ê¸°ìˆ , ê·¼ë¬´ì§€ì—­, ê·¼ë¡œì¡°ê±´, ëª¨ì§‘ê¸°ê°„, ë§í¬, ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, ë³µì§€_ë°_í˜œíƒ, ì±„ìš©ì ˆì°¨, í•™ë ¥, ê·¼ë¬´ì§€ì—­_ìƒì„¸, ë§ˆê°ì¼ì)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        try:
            cursor.execute(insert_query, (title, company_name, skill, loc, condition, date, job_url, *details.values()))
            db.commit()
            saved_job_count += 1  # ì €ì¥ëœ ê³µê³  ì¹´ìš´íŠ¸ ì¦ê°€
        except Exception as e:
            print(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            db.rollback()
    
    cursor.close()
    print(f"ì´ {saved_job_count}ê°œì˜ ê³µê³  DB ì €ì¥ ì™„ë£Œ!")

def main():
    print("ì±„ìš© ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ì¤‘...")
    job_data = scrape_jobs()
    
    if job_data:
        save_to_db(job_data)
    else:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    create_saved_jobs_table()
    main()
